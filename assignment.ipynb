{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Initial Variables, You can change these\n",
    "gridx = 10\n",
    "gridy = 10\n",
    "obstaclepercent = 50\n",
    "\n",
    "#amount of obstacles\n",
    "obstacleamount = math.floor((((gridx * gridy) - 2) * (obstaclepercent/100)))\n",
    "\n",
    "#symbols\n",
    "symbolstart = 'S'\n",
    "symbolend = 'G'\n",
    "symbolopen = 'O'\n",
    "symbolobstacle = 'H'\n",
    "\n",
    "\n",
    "\n",
    "#randomise start and end\n",
    "def randomcoord():\n",
    "    return (random.randint(0, gridx-1), random.randint(0, gridy-1))\n",
    "#start and end location\n",
    "start = randomcoord()\n",
    "end = randomcoord()\n",
    "#if the start and end location are the same\n",
    "while start == end:\n",
    "    end = randomcoord()\n",
    "\n",
    "#start = (6, 9)\n",
    "#end = (4, 5)\n",
    "\n",
    "#start = (5, 3)\n",
    "#end = (7, 8)\n",
    "\n",
    "#solution path\n",
    "solutionpath = list(tuple())\n",
    "\n",
    "#if the goal is near the solution\n",
    "flag = False\n",
    "if start[0] == end[0]:\n",
    "    if end[1] > 0:\n",
    "        if start[1] == (end[1] + 1):\n",
    "            flag = True\n",
    "    if end[1] < gridy:\n",
    "        if start[1] == (end[1] - 1):\n",
    "            flag = True\n",
    "if start[1] == end[1]: \n",
    "    if end[0] > 0:\n",
    "        if start[0] == (end[0] + 1):\n",
    "            flag = True\n",
    "    if end[0] < gridx:\n",
    "        if start[0] == (end[0] - 1):\n",
    "            flag = True\n",
    "            \n",
    "           \n",
    "#finds a solution path. simple rightangled approach\n",
    "if flag == False:\n",
    "    if start[0] < end[0]:\n",
    "        for x in range(start[0]+1,end[0]+1):\n",
    "            solutionpath.append((x,start[1]))\n",
    "    else:\n",
    "        for x in range(start[0]-1,end[0]-1,-1):\n",
    "            solutionpath.append((x,start[1]))\n",
    "    if abs(start[1] - end[1]) >= 1:\n",
    "        if start[1] < end[1]: \n",
    "            for y in range(start[1],end[1]-1):\n",
    "                solutionpath.append((end[0],y+1))\n",
    "        else:\n",
    "            for y in range(start[1],end[1]+1,-1):\n",
    "                solutionpath.append((end[0],y-1))\n",
    "                \n",
    "    else:\n",
    "        solutionpath = solutionpath[:-1]\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "#defining the grid and adding the start/end\n",
    "thegrid = [[symbolopen]*gridy for i in range(gridx)]    \n",
    "#thegrid[start[0]][start[1]] = symbolstart\n",
    "thegrid[end[0]][end[1]] = symbolend\n",
    "\n",
    "\n",
    "\n",
    "#defining solution path\n",
    "for coord in solutionpath:\n",
    "    thegrid[coord[0]][coord[1]] = '*' \n",
    "    \n",
    "\n",
    "    \n",
    "#defining a list of possible obstacles\n",
    "obstaclearray = []\n",
    "for coord_x in range(0, gridx):\n",
    "    for coord_y in range(0, gridy):\n",
    "        if thegrid[coord_x][coord_y] == symbolopen:\n",
    "            obstaclearray.append((coord_x,coord_y))\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "while obstacleamount > 0: \n",
    "    choice = np.random.randint(0,len(obstaclearray))\n",
    "    thegrid[obstaclearray[choice][0]][obstaclearray[choice][1]] = symbolobstacle\n",
    "    obstaclearray.pop(choice)\n",
    "    obstacleamount -= 1\n",
    "\n",
    "           \n",
    "        \n",
    "        \n",
    "\n",
    "for coord in solutionpath:\n",
    "    thegrid[coord[0]][coord[1]] = symbolopen        \n",
    "        \n",
    "    \n",
    "\n",
    "'''\n",
    "#prints grid    \n",
    "for y in range(0,gridx):\n",
    "    print(thegrid[y])\n",
    "    \n",
    "print(start)\n",
    "print(end)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "file1 = open(\"gridworld.txt\",\"w\")\n",
    "for y in range(0,gridx):\n",
    "    file1.write(''.join(thegrid[y])+\"\\n\")\n",
    "file1.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# maze example\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\" Grid World environment\n",
    "            there are four actions (left, right, up, and down) to move an agent\n",
    "            In a grid, if it reaches a goal, it get 30 points of reward.\n",
    "            If it falls in a hole or moves out of the grid world, it gets -5.\n",
    "            Each step costs -1 point. \n",
    "\n",
    "        to test GridWorld, run the following sample codes:\n",
    "\n",
    "            env = GridWorld('grid.txt')\n",
    "\n",
    "            env.print_map()\n",
    "            print [2,3], env.check_state([2,3])\n",
    "            print [0,0], env.check_state([0,0])\n",
    "            print [3,4], env.check_state([3,4])\n",
    "            print [10,3], env.check_state([10,3])\n",
    "\n",
    "            env.init([0,0])\n",
    "            print env.next(1)  # right\n",
    "            print env.next(3)  # down\n",
    "            print env.next(0)  # left\n",
    "            print env.next(2)  # up\n",
    "            print env.next(2)  # up\n",
    "\n",
    "        Parameters\n",
    "        ==========\n",
    "        _map        ndarray\n",
    "                    string array read from a file input\n",
    "        _size       1d array\n",
    "                    the size of _map in ndarray\n",
    "        goal_pos    tuple\n",
    "                    the index for the goal location\n",
    "        _actions    list\n",
    "                    list of actions for 4 actions\n",
    "        _s          1d array\n",
    "                    current state\n",
    "    \"\"\"\n",
    "    def __init__(self, fn):\n",
    "        # read a map from a file\n",
    "        self._map = self.read_map(fn)\n",
    "        self._size = np.asarray(self._map.shape)\n",
    "        self.goal_pos = np.where(self._map == 'G')\n",
    "\n",
    "        # definition of actions (left, right, up, and down repectively)\n",
    "        self._actions = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        self._s = None\n",
    "\n",
    "    def get_cur_state(self):\n",
    "        return self._s\n",
    "\n",
    "    def get_size(self):\n",
    "        return self._size\n",
    "\n",
    "    def read_map(self, fn):\n",
    "        grid = []\n",
    "        with open(fn) as f:\n",
    "            for line in f:\n",
    "               grid.append(list(line.strip()))\n",
    "        return np.asarray(grid)\n",
    "\n",
    "    def print_map(self):\n",
    "        print( self._map )\n",
    "\n",
    "    def check_state(self, s):\n",
    "        if isinstance(s, collections.Iterable) and len(s) == 2:\n",
    "            if s[0] < 0 or s[1] < 0 or\\\n",
    "               s[0] >= self._size[0] or s[1] >= self._size[1]:\n",
    "               return 'N'\n",
    "            return self._map[tuple(s)].upper()\n",
    "        else:\n",
    "            return 'F'  # wrong input\n",
    "\n",
    "    def init(self, state=None):\n",
    "        if state is None:\n",
    "            s = [0, 0]\n",
    "        else:\n",
    "            s = state\n",
    "\n",
    "        if self.check_state(s) == 'O':\n",
    "            self._s = np.asarray(state)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid state for init\")\n",
    "\n",
    "    def next(self, a):\n",
    "        s1 = self._s + self._actions[a]\n",
    "        # state transition\n",
    "        curr = self.check_state(s1)\n",
    "        \n",
    "        if curr == 'H' or curr == 'N':\n",
    "            return -5\n",
    "        elif curr == 'F':\n",
    "            warnings.warn(\"invalid state \" + str(s1))\n",
    "            return -5\n",
    "        elif curr == 'G':\n",
    "            self._s = s1\n",
    "            return 30\n",
    "        else:\n",
    "            self._s = s1\n",
    "            return -1\n",
    "        \n",
    "    def is_goal(self):\n",
    "        return self.check_state(self._s) == 'G'\n",
    "            \n",
    "    def get_actions(self):\n",
    "        return self._actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O' 'H' 'H' 'O' 'H' 'H' 'H' 'O' 'H' 'H']\n",
      " ['O' 'O' 'H' 'H' 'H' 'O' 'H' 'H' 'O' 'O']\n",
      " ['O' 'O' 'H' 'H' 'O' 'H' 'H' 'H' 'O' 'O']\n",
      " ['H' 'O' 'O' 'O' 'O' 'O' 'H' 'H' 'H' 'O']\n",
      " ['H' 'H' 'O' 'H' 'O' 'O' 'O' 'O' 'G' 'O']\n",
      " ['O' 'H' 'O' 'H' 'O' 'H' 'O' 'O' 'H' 'O']\n",
      " ['O' 'H' 'H' 'H' 'O' 'H' 'O' 'O' 'O' 'H']\n",
      " ['O' 'H' 'O' 'O' 'O' 'H' 'O' 'H' 'H' 'O']\n",
      " ['H' 'H' 'O' 'O' 'O' 'O' 'O' 'H' 'H' 'O']\n",
      " ['O' 'H' 'H' 'H' 'O' 'H' 'H' 'H' 'H' 'H']]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(\"gridworld.txt\")\n",
    "env.print_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top-left to (0,0)\n",
    "def coord_convert(s, sz):\n",
    "    return [s[1], sz[0]-s[0]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent: \n",
    "    \"\"\"\n",
    "        Reinforcement Learning Agent Model for training/testing\n",
    "        with Tabular function approximation\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.size = env.get_size()\n",
    "        print(self.size)\n",
    "        self.n_a = len(env.get_actions())\n",
    "        # self.Q table including the surrounding border\n",
    "        self.Q = np.zeros((self.size[0], self.size[1], self.n_a))\n",
    "        \n",
    "    def greedy(self, s):\n",
    "        return np.argmax(self.Q[s[0], s[1]])\n",
    "\n",
    "    def epsilon_greed(self, e,s):\n",
    "        if np.random.rand() < e:\n",
    "            return np.random.randint(self.n_a)\n",
    "        else:\n",
    "            return self.greedy(s)\n",
    "    \n",
    "    def train(self, start, **params):\n",
    "        \n",
    "        # parameters\n",
    "        gamma = params.pop('gamma', 0.99)\n",
    "        alpha = params.pop('alpha', 0.1)\n",
    "        epsilon= params.pop('epsilon', 0.1)\n",
    "        maxiter= params.pop('maxiter', 1000) \n",
    "        maxstep= params.pop('maxstep', 1000)\n",
    "        \n",
    "        # init self.Q matrix\n",
    "        self.Q[...] = 0\n",
    "        self.Q[self.env._map == 'H'] = -np.inf\n",
    "        \n",
    "        # online train\n",
    "        # rewards and step trace\n",
    "        rtrace = []\n",
    "        steps = []\n",
    "        for j in range(maxiter):\n",
    "\n",
    "            self.env.init(start)\n",
    "            s = self.env.get_cur_state()\n",
    "            # selection an action\n",
    "            a = self.epsilon_greed(epsilon, s)\n",
    "\n",
    "            rewards = []\n",
    "            trace = np.array(coord_convert(s, self.size))\n",
    "            # run simulation for max number of steps \n",
    "            for step in range(maxstep):\n",
    "                # move\n",
    "                r = self.env.next(a)\n",
    "                s1 = self.env.get_cur_state()\n",
    "                print(s1)\n",
    "                a1 = self.epsilon_greed(epsilon, s1)\n",
    "\n",
    "                rewards.append(r)\n",
    "                trace = np.vstack((trace, coord_convert(s1, self.size)))\n",
    "            \n",
    "                # update self.Q table \n",
    "                self.Q[s[0],s[1],a] += alpha*(r + gamma * np.max(self.Q[s1[0],s1[1],:]) - self.Q[s[0],s[1],a])\n",
    "                \n",
    "                if self.env.is_goal(): # reached the goal\n",
    "                    self.Q[s1[0], s1[1], a1] = 0\n",
    "                    break\n",
    "\n",
    "                s = s1\n",
    "                a = a1\n",
    "\n",
    "            rtrace.append(np.sum(rewards))\n",
    "            steps.append(step+1)\n",
    "        return rtrace, steps, trace # last trace of trajectory\n",
    "\n",
    "    def test(self, start, maxstep=1000):\n",
    "        env.init(start)\n",
    "        s = env.get_cur_state()\n",
    "        a = np.argmax(self.Q[s[0], s[1], :])\n",
    "        trace = np.array(coord_convert(s, self.size))\n",
    "        for step in range(maxstep):\n",
    "            env.next(a)\n",
    "            s1 = env.get_cur_state()\n",
    "            a1 = np.argmax(self.Q[s1[0], s1[1],:])\n",
    "            trace = np.vstack((trace, coord_convert(s1, self.size)))\n",
    "            if env.is_goal():  # reached the goal\n",
    "                break\n",
    "            a = a1\n",
    "        \n",
    "        return trace            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting tools \n",
    "\n",
    "def plot_trace(agent, start, trace, title=\"test trajectory\"):\n",
    "    plt.plot(trace[:, 0], trace[:, 1], \"ko-\")\n",
    "    plt.text(env.goal_pos[1], agent.size[0]-env.goal_pos[0]-1, 'G')\n",
    "    plt.text(start[1], agent.size[0]-start[0]-1, 'S')\n",
    "    plt.xlim([0, agent.size[1]])\n",
    "    plt.ylim([0, agent.size[0]])\n",
    "    \n",
    "\n",
    "def plot_train(agent, rtrace, steps, trace, start):\n",
    "\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax = fig.add_subplot(221)\n",
    "    plt.plot(rtrace)\n",
    "    plt.ylabel(\"sum of rewards\")\n",
    "\n",
    "    ax1 = fig.add_subplot(222)\n",
    "    plt.plot(steps)\n",
    "    plt.ylabel(\"# steps\")\n",
    "\n",
    "    # contour plot for agent.Q\n",
    "    ax2 = fig.add_subplot(223)\n",
    "    xs = range(agent.size[1])\n",
    "    ys = range(agent.size[0])\n",
    "    maxQ = np.max(agent.Q, axis=2)\n",
    "    h_b = (maxQ==-np.inf)\n",
    "    maxQ[h_b] = 0\n",
    "    maxQ[h_b] = np.min(maxQ) - 100\n",
    "    cs = plt.contourf(xs, ys[::-1], maxQ)\n",
    "    plt.colorbar(cs)\n",
    "    plt.text(env.goal_pos[1], agent.size[0]-env.goal_pos[0]-1, 'G')\n",
    "    plt.text(start[1], agent.size[0]-start[0]-1, 'S')\n",
    "    plt.ylabel(\"max agent.Q\")\n",
    "\n",
    "    # plot traces\n",
    "    ax3 = fig.add_subplot(224)\n",
    "    plot_trace(agent, start, trace, \"trace of the last episode\")\n",
    "\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds starting point 'S' in grid world to get x and y values for RL agent\n",
    "def getstart(gs):\n",
    "    xy = []\n",
    "    x = gs._map.shape[0]\n",
    "    y = gs._map.shape[1]\n",
    "    for a in range(0, x):\n",
    "        for b in range(0, y):\n",
    "            if (gs._map[a][b] == 'S'):\n",
    "                print(\"Found S at:\", a, b)\n",
    "                xy.append(a)\n",
    "                xy.append(b)\n",
    "                return xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 10]\n",
      "[4, 9]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 9]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[3 9]\n",
      "[3 9]\n",
      "[3 9]\n",
      "[3 9]\n",
      "[3 9]\n",
      "[2 9]\n",
      "[2 8]\n",
      "[2 8]\n",
      "[2 8]\n",
      "[2 9]\n",
      "[2 9]\n",
      "[2 9]\n",
      "[1 9]\n",
      "[1 8]\n",
      "[1 8]\n",
      "[1 8]\n",
      "[1 9]\n",
      "[1 9]\n",
      "[1 9]\n",
      "[1 9]\n",
      "[1 9]\n",
      "[2 9]\n",
      "[3 9]\n",
      "[4 9]\n",
      "[4 9]\n",
      "[4 8]\n",
      "[3 9]\n",
      "[4 9]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[5 9]\n",
      "[5 9]\n",
      "[5 9]\n",
      "[5 9]\n",
      "[5 9]\n",
      "[4 9]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[5 9]\n",
      "[4 9]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 9]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[3 9]\n",
      "[2 9]\n",
      "[2 8]\n",
      "[1 8]\n",
      "[1 8]\n",
      "[1 8]\n",
      "[2 8]\n",
      "[2 8]\n",
      "[2 8]\n",
      "[2 9]\n",
      "[1 9]\n",
      "[1 8]\n",
      "[1 9]\n",
      "[2 9]\n",
      "[3 9]\n",
      "[4 9]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 9]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[3 9]\n",
      "[2 9]\n",
      "[3 9]\n",
      "[4 9]\n",
      "[4 8]\n",
      "[3 9]\n",
      "[4 9]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[5 9]\n",
      "[4 9]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[3 9]\n",
      "[4 9]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[3 9]\n",
      "[4 9]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n",
      "[4 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-65e3027d66f2>:67: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  if isinstance(s, collections.Iterable) and len(s) == 2:\n"
     ]
    }
   ],
   "source": [
    "agent = RLAgent(env)\n",
    "start = [start[1],start[0]]\n",
    "print(start)\n",
    "rtrace, steps, trace = agent.train(start, \n",
    "                                   gamma=0.99, \n",
    "                                   alpha=0.1, \n",
    "                                   epsilon=0.1, \n",
    "                                   maxiter=100, \n",
    "                                   maxstep=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
